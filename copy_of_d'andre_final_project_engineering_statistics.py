# -*- coding: utf-8 -*-
"""Copy of D'Andre Final Project Engineering Statistics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IQN52B4Ip98TqieQGBvwFOb3p1zmDWlG

# Final Project Notebook - Spring 2024

# This final project will demonstrate how modeling software can be used to examine and sort data.  AlexNet trains on thousands of images and is used to classify them into categories based on the content of the image.  I will sort between different types of cars using AlexNet in this final project.

The first cell imports libraries of prewritten python code.

Libraries give the program access to custom functions written by others.

Here are some common libraries and their usages:

1. MatPlotLib - Generates plots and graphs using data
2. ImageIO - Allows importing and easily editing images
3. Torch - Framework for Neural Networking, to normalize data for use by different neural networks including AlexNet
4. Numpy (Numerical Python) - Contains a large amount of non-default python mathematical operations for streamlined computation
"""

import matplotlib.pyplot as plt #import libraries
import imageio
import torch
import torchvision
from torchvision import models, transforms
import numpy as np
from torchvision.models import *
from PIL import Image
import requests
from torchvision import models
from torchsummary import summary

"""**FUNCTIONS:** Functions are reusable sections of code that can be referenced for specific tasks.  Functions can take inputs and do operations, and can give outputs.

*Plot:* A custom plot function lets the program set defaults in the settings of MatPlotLib and save them to be reused.
"""

def plot(x): #name of the function and the input variable
    fig, ax = plt.subplots() #retrieving the default plotting area from the MatPlotLib library
    im = ax.imshow(x,cmap='gray') #putting the input into the plotting area, changing the colormap to black and white, saving it to a variable "im"
    ax.axis('off') #turning the axis off
    fig.set_size_inches(20, 20) #setting the size of the plot area
    plt.show() #displaying the plot in the notebook

"""The ImageIO library has functions to read the image stored at a specific url."""

im = imageio.imread('https://raw.githubusercontent.com/imageio/imageio-binaries/master/images/imageio_banner.png') #getting the image from the url

"""The program stores the image to the variable IM, and the function "plot()" can be used to display the image stored there."""

plot(im) #displaying the image

"""AlexNet is open source - Anyone can use it in their program"""

net = alexnet(pretrained=True).cuda(0) #saving the downloaded version of alexnet into a variable
#getting the pretrained version
#sending it to the Google GPU using the cuda method

"""All data needs to be normalized for a neural network to understand it.  A group of images with different brightness levels, contrast, etc. will be harder if not impossible for a neural network to understand."""

normalize = transforms.Normalize( #setting the defaults for image transforms, forcing color channels to have the same mean value and standard deviation
   mean=[0.485, 0.456, 0.406],
   std=[0.229, 0.224, 0.225]
)
preprocess = transforms.Compose([ #setting all the images to the same size, and cropping them to be square
   transforms.Resize(256),
   transforms.CenterCrop(224),
   transforms.ToTensor(),
   normalize
])

im = imageio.imread('https://www.medicalnewstoday.com/content/images/articles/322/322868/golden-retriever-puppy.jpg') #getting a new image from a url

plot(im) #displaying the new image using the plot() function

"""**Objects:** An object is similar to a variable, but has "methods."  Objects store data but also use methods (built-in functions) to change, reshape, or otherwise change the stored data."""

image = Image.fromarray(im) #converting the image to an Image object from the Image library, converting using the fromarray method

img_tensor = preprocess(image) #using the preprocess method to normalize the image and its color channels

img_tensor = img_tensor.unsqueeze_(0) #using the unsqueeze method to shift the data over by one dimension

img_tensor.shape #calling the shape method

"""By putting images and data into Torch objects makes it possible to use common neural networking methods on the data."""

img_variable = torch.tensor(img_tensor).cuda(0) #creating a copy of our image as a torch object, sending it to the graphics card

"""Once data is formatted as a Torch object neural networks such as AlexNet can be run on the data.

AlexNet's output is a Vector, also known as a Classifier.  Each point along the vector represents a different classification of image.
"""

out = net(img_variable) #saving the output to a variable "out"

label_index = out.cpu().data.numpy().argmax() #saving the point where the highest correlation of the image lies

label_index #showing that the point is point 215 along the vector

"""The max value outputs the highest correlation - searching along the whole vector allows finding the top ten values as well."""

top_list = np.flip(np.argsort(out.cpu().data.numpy())[0][-10:]) #saving the 10 points with highest image correlation

"""Downloading the label key from AlexNet's website allows the result vector to be interpreted."""

LABELS_URL = 'https://s3.amazonaws.com/mlpipes/pytorch-quick-start/labels.json' #getting the url of alexnet labels, saving it to variable

labels = {int(key):value for (key, value) in requests.get(LABELS_URL).json().items()} #converting the data stored in the url as a library of labels for each part of the vector

"""The label key is used to show the names of the points along the vector."""

print(labels[label_index]) #guessing the content of the image, the best guess is a Brittany Spaniel Dog

for i in range(10): #printing out the top 10 guesses of Alexnet as to what the image is
    print(labels[top_list[i]])

"""Alexnet is composed of layers.  Each layer slightly modifies the input for interpretation by the next layer.  Once the data has been shrunk down, the classifier layers send guesses as to what the data represents."""

net #seeing the different layers of Alexnet

"""Each layer modifies the size of the tensor input, until what was initially a very large tensor, becomes a relatively small classifier."""

summary(net, (3, 224, 224)) #changing the shape of the data as the data passes through Alexnet

out = net.features[0](img_variable).cpu().detach().numpy() #getting the image after passing through the first layer of alexnet

plot(out[0,0,:,:]) #seeing how much the data changes in just the first pass through
#getting rid of extra data helps alexnet focus on only the DEFINING features of an image

"""After passing through the neural network and becoming a vector, the image is unrecognizable.  This is similar to how an eye processes light down into electrical signals in the brain, and the mind then assigns meaning to those signals."""

plt.plot(np.arange(4096),net.classifier[0:6](net.avgpool(net.features[0:13](img_variable)).flatten()).cpu().detach().numpy()) #getting the first classifier
fig = plt.gcf()
fig.set_size_inches(10, 10) #plotting the classifier vector

im = imageio.imread('http://bocasurfcam.com/most_recent_image.php') #saving a new image to the im variable, from boca surf camera

plot(im) #showing the image saved

"""**Load_IM:** This function's purpose is to do all the preprocessing work in one function; calling the function will make the image ready for use by alexnet"""

def load_im(im): #function name
    image = Image.fromarray(im) #converting to pil
    img_tensor = preprocess(image) #preprocessing the data using the defaults set
    img_tensor = img_tensor.unsqueeze_(0) #dimension shifting the tensor
    img_variable = torch.tensor(img_tensor).cuda(0) #setting to a torch object and sending to the graphics card where alexnet is loaded onto
    return img_variable #outputting the resulting tensor, for saving or running through alexnet

"""By making the function load_im, the code necessary to get from a url to an alexnet classifier has been greatly shortened."""

out = net(load_im(im)) #using the function, saving the classifier to the out variable

"""**Inference:** This function will take an torch tensor input, run through alexnet to get the classifier, find the top 10 correlated values in it, and output both the best guess and the top ten guesses."""

def inference(im): #function name, input tensor
    out = net(load_im(im)) #saving the classifier result from alexnet locally
    label_index = out.cpu().data.numpy().argmax() #finding the best guess value
    top_list = np.flip(np.argsort(out.cpu().data.numpy())[0][-10:]) #finding the top ten guess values
    print(labels[label_index]) #printing the label of the best guess
    print('____') #printing a line
    for i in range(10): #for loop printing the top 10 guesses
        print(labels[top_list[i]])

"""Making a few functions and using prebuilt libraries reduces the necessary code for repeatedly classifying images to just a single function with a single input."""

inference(im) #running the inference function with our image of the beach as the input, everything else is neatly packaged away

"""# Restart Notebook (Disconnect and Delete Runtime) Before Running Next Section

# Custom Data Deck
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install wandb #w and b stands for weights and biases, it's used to update alexnet based on training
# !apt-get install poppler-utils
# !pip install pdf2image
# !pip install flashtorch
# import requests
# from pdf2image import convert_from_path
# import matplotlib.pyplot as plt #plotting in the document
# import numpy as np #numerical python
# import torch #neural networking framework
# import requests
# from torchvision import *
# from torchvision.models import *
# from flashtorch.utils import apply_transforms
# import wandb as wb #neural network trainer

"""**GPU:** This function converts arrays into Torch objects, and sends them to the graphics card, where AlexNet is loaded."""

def GPU(data):
    return torch.tensor(data, requires_grad=True, dtype=torch.float, device=torch.device('cuda'))

def GPU_data(data):
    return torch.tensor(data, requires_grad=False, dtype=torch.float, device=torch.device('cuda'))

"""**Plot:** This function allows quick matplotlib image display"""

def plot(x):
    fig, ax = plt.subplots() #creating plotting area
    im = ax.imshow(x, cmap = 'gray') #putting image onto the plotting area with gray colormap
    ax.axis('off') #turning off axes
    fig.set_size_inches(5, 5) #setting default size
    plt.show() #displaying the plot

"""**Get_Google_Slides:** These two functions will divide the url up (get_google_slides) and convert each slide from a pdf to an image (get_slides)

**Load:** This function normalizes the slides to prepare to run through alexnet
"""

def get_google_slide(url): #inputting the google slides url
    url_head = "https://docs.google.com/presentation/d/" #setting default url header
    url_body = url.split('/')[5] #splitting the url to get unique slide identifier
    page_id = url.split('.')[-1] #splitting the url to get unique page identifier
    return url_head + url_body + "/export/pdf?id=" + url_body + "&pageid=" + page_id #constructing a new url for the pdf version of the slide

def get_slides(url): #inputting the google slides url
    url = get_google_slide(url) #converting it to the pdf url using the previous function
    r = requests.get(url, allow_redirects=True) #downloading the pdf to this notebook
    open('file.pdf', 'wb').write(r.content) #opening the pdf in the notebook
    images = convert_from_path('file.pdf', 500) #saving the slides to a variable
    return images #returning that variable

def load(image): #inputting the saved images

    return apply_transforms(image).clone().detach().requires_grad_(True).to(device) #normalizing using the transforms from the wandb library

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") #setting the default device to the GPU, not the CPU

"""A dictionary of the labels which define the classifier:"""

labels = {int(key):value for (key, value) in requests.get('https://s3.amazonaws.com/mlpipes/pytorch-quick-start/labels.json').json().items()} #getting the labels from the alexnet site

model = alexnet(weights='DEFAULT').to(device) #downloading alexnet to the notebook
model.eval();

url = "https://docs.google.com/presentation/d/1tlFULpPrI1j8vzVIRzrgrovdpKjaGbhpLyN7DcVPEOA/edit#slide=id.g2b78f466460_0_310" #saving the url of the slides to this notebook

images = [] #creating an empty images list

for image in get_slides(url): #using a for loop to populate the images list with the slides from the url

    plot(image)

    images.append(load(image))

images = torch.vstack(images) #vertically stack the images in torch

images.shape #seeing the size of our data, 50 slides, 3 color channels each, all 224x224 pixels

"""**Model:** this function sends data through alexnet.  At the beginning there are 7.5 million individual values in our tensor, all shrunk down into a mere 50 classifier vectors."""

model(images) #showing what our data looks like after it is converted into classifiers

y = model(images) #saving our classifiers to a variable

y.shape #seeing that the size has significantly reduced

"""The guesses variable is saving the maximum value in each classifier to give alexnet's choice."""

guesses = torch.argmax(y, 1).cpu().numpy()

"""The classifier maxima gives the guesses of what Alexnet thinks each image is based on its classifier."""

for i in list(guesses):
    print(labels[i]) #printing the guesses out

"""Creating a key: By making a list of length 50 and seperating the first and second half, we will show alexnet that we are giving it two types of images, and update its training based on the key."""

Y = np.zeros(50,) #creating a key of length 50
Y[25:] = 1 #changing the last 25 values to 1

Y

X = y.detach().cpu().numpy() #getting the values of guesses into each index of the key

X.shape

plt.plot(X[0],'.') #plotting the first value on the key to see a visualization of alexnet guesses

X[0] #printing all 1000 values out

np.argmax(X[0]) #finding the index of the biggest one

labels[479] #getting the label

top_ten = np.argsort(X[0])[::-1][0:10] #getting the top 10 values

for i in top_ten: #printing the corresponding labels
    print(labels[i])

labels #printing the entire dictionary of labels

plt.hist(X[0]) #plotting a histogram of the classifier, very few values deviate from zero, those values which do are the guesses

X = GPU_data(X) #send the data and key to torch
Y = GPU_data(Y)

"""**Softmax:** For neural networking, a softmax function transform values into probabilities.  This helps make things like classification easier."""

def softmax(x):
    s1 = torch.exp(x - torch.max(x,1)[0][:,None]) #this is the definition of a softmax function
    s = s1 / s1.sum(1)[:,None]
    return s #returning a map of probabilities to compare against the key

"""**Cross-Entropy:** For neural networking, cross-entropy, or loss, is a function used to compare a probability distribution to the predicted result, the more dissimilar, the higher the loss, which helps the neural network adjust the weights and biases by the correct amount."""

def cross_entropy(outputs, labels):
    return -torch.sum(softmax(outputs).log()[range(outputs.size()[0]), labels.long()])/outputs.size()[0] #this is the definition of the loss function

"""**Truncated Normal Distribution:** A normal distribution is used for the initial weights and biases, but to prevent slow learning, "truncated" normal distributions have an upper and lower bound due to the finite limitations of computing."""

def Truncated_Normal(size):

    u1 = torch.rand(size)*(1-np.exp(-2)) + np.exp(-2)
    u2 = torch.rand(size)
    z  = torch.sqrt(-2*torch.log(u1)) * torch.cos(2*np.pi*u2) #this is the equation for our truncated normal function

    return z

"""**Accuracy:** Accuracy is simply a percentage which shows the rate at which a neural network correctly classifies data.  Tracking it over time helps seeing the progress of a network's learning."""

def acc(out,y):
    with torch.no_grad():
        return (torch.sum(torch.max(out,1)[1] == y).item())/y.shape[0] #correct guesses/total guesses

X.shape

def get_batch(mode): #defining the function
    b = c.b #the size of the batch
    if mode == "train":
        r = np.random.randint(X.shape[0]-b) #setting default weights and biases and giving a guess based on the probability dist, then comparing to training key
        x = X[r:r+b,:]
        y = Y[r:r+b]
    elif mode == "test":
        r = np.random.randint(X_test.shape[0]-b) #setting default weights and biases and giving a guess based on weights and biases
        x = X_test[r:r+b,:]
        y = Y_test[r:r+b]
    return x,y

def model(x,w): #using matrix multiplication to generate classifier

    return x@w[0]

def make_plots(): #track accuracy over time

    acc_train = acc(model(x,w),y)

    wb.log({"acc_train": acc_train})

wb.init(project="Linear_Model_Photo_1");
c = wb.config

c.h = 0.001
c.b = 4 #batch sizes
c.epochs = 100000 #runs

w = [GPU(Truncated_Normal((1000,2)))] #generate normally distributed weights

optimizer = torch.optim.Adam(w, lr=c.h) #use built-in torch optimizer (Adam)

for i in range(c.epochs):

    x,y = get_batch('train') #set mode to train

    loss = cross_entropy(softmax(model(x,w)),y) #calculate loss

    optimizer.zero_grad() #reset optimizer
    loss.backward() #adjust weights
    optimizer.step() #move to next run in the optomizer

    wb.log({"loss": loss}) #send loss to wandb

    make_plots() #generate graphs





